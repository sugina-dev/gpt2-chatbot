import argparse
import copy
import opencc2
import os
import threading
import time
import torch
import torch.nn.functional as F
from transformers import BertTokenizer
from transformers.modeling_gpt2 import GPT2LMHeadModel

def set_interact_args():
	parser = argparse.ArgumentParser()
	parser.add_argument('--device', default='0', type=str, required=False, help='ç”Ÿæˆè®¾å¤‡')
	parser.add_argument('--temperature', default=1, type=float, required=False, help='ç”Ÿæˆçš„temperature')
	parser.add_argument('--topk', default=8, type=int, required=False, help='æœ€é«˜ké€‰1')
	parser.add_argument('--topp', default=0, type=float, required=False, help='æœ€é«˜ç§¯ç´¯æ¦‚ç‡')
	parser.add_argument('--model_config', default='config/model_config_dialogue_small.json', type=str, required=False, help='æ¨¡å‹å‚æ•°')
	parser.add_argument('--voca_path', default='vocab_small.txt', type=str, required=False, help='é€‰æ‹©è¯åº“')
	parser.add_argument('--dialogue_model_path', default='dialogue_model/', type=str, required=False, help='dialogue_modelè·¯å¾„')
	parser.add_argument('--mmi_model_path', default='mmi_model/', type=str, required=False, help='äº’ä¿¡æ¯mmi_modelè·¯å¾„')
	parser.add_argument('--repetition_penalty', default=1.5, type=float, required=False, help='é‡å¤æƒ©ç½šå‚æ•°ï¼Œè‹¥ç”Ÿæˆçš„å¯¹è¯é‡å¤æ€§è¾ƒé«˜ï¼Œå¯é€‚å½“æé«˜è¯¥å‚æ•°')
	parser.add_argument('--seed', type=int, default=None, help='è®¾ç½®ç§å­ç”¨äºç”Ÿæˆéšæœºæ•°ï¼Œä»¥ä½¿å¾—è®­ç»ƒçš„ç»“æœæ˜¯ç¡®å®šçš„')
	parser.add_argument('--max_len', type=int, default=25, help='æ¯ä¸ªutteranceçš„æœ€å¤§é•¿åº¦,è¶…è¿‡æŒ‡å®šé•¿åº¦åˆ™è¿›è¡Œæˆªæ–­')
	parser.add_argument('--max_history_len', type=int, default=5, help='dialogue historyçš„æœ€å¤§é•¿åº¦')
	parser.add_argument('--no_cuda', action='store_true', help='ä¸ä½¿ç”¨GPUè¿›è¡Œé¢„æµ‹')
	parser.add_argument('--batch_size', type=int, default=5, help='æ‰¹é‡ç”Ÿæˆresponseï¼Œç„¶åç»è¿‡MMIæ¨¡å‹è¿›è¡Œç­›é€‰')
	parser.add_argument('--debug', action='store_true', help='æŒ‡å®šè¯¥å‚æ•°ï¼Œå¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„æ‰€æœ‰å€™é€‰çš„reponseï¼ŒåŠå…¶loss')
	return parser.parse_args()

def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
	''' Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
		Args:
			logits: logits distribution shape (vocabulary size)
			top_k > 0: keep only top k tokens with highest probability (top-k filtering).
			top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).
				Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)
	'''
	assert logits.dim() == 2
	top_k = min(top_k, logits[0].size(-1))  # Safety check
	if top_k > 0:
		# Remove all tokens with a probability less than the last token of the top-k
		# torch.topk()è¿”å›æœ€åä¸€ç»´æœ€å¤§çš„top_kä¸ªå…ƒç´ ï¼Œè¿”å›å€¼ä¸ºäºŒç»´(values,indices)
		# ...è¡¨ç¤ºå…¶ä»–ç»´åº¦ç”±è®¡ç®—æœºè‡ªè¡Œæ¨æ–­
		for logit in logits:
			indices_to_remove = logit < torch.topk(logit, top_k)[0][..., -1, None]
			logit[indices_to_remove] = filter_value  # å¯¹äºtopkä¹‹å¤–çš„å…¶ä»–å…ƒç´ çš„logitså€¼è®¾ä¸ºè´Ÿæ— ç©·

	if top_p > 0.0:
		sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)  # å¯¹logitsè¿›è¡Œé€’å‡æ’åº
		cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

		# Remove tokens with cumulative probability above the threshold
		sorted_indices_to_remove = cumulative_probs > top_p
		# Shift the indices to the right to keep also the first token above the threshold
		sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
		sorted_indices_to_remove[..., 0] = 0
		for index, logit in enumerate(logits):
			indices_to_remove = sorted_indices[index][sorted_indices_to_remove[index]]
			logit[indices_to_remove] = filter_value
	return logits

class InidvidualDialog:
	lock = threading.RLock()
	lock_mmi = threading.RLock()

	def __init__(self):
		self.history = []

	def response(self, text):
		# è¼¸å…¥å­—ç¬¦ä¸²è½‰ç°¡é«”
		text = opencc_simp.convert(text)
		text = text.replace('å–«', 'åƒ')
		# å°‡ç°¡é«”å­—ç¬¦ä¸²å­˜å…¥æ­·å²
		self.history.append(tokenizer.encode(text))
		# ç”±æ¨¡å‹æ ¹æ“šæ­·å²å¾—å‡ºå¤šå€‹å€™é¸è§£
		InidvidualDialog.lock.acquire()
		candidate_responses = get_response(self.history)
		InidvidualDialog.lock.release()
		# ç”± MMI é¸å‡ºä¸€å€‹æœ€å„ªè§£
		InidvidualDialog.lock_mmi.acquire()
		best_response = mmi_choice(self.history, candidate_responses)
		InidvidualDialog.lock_mmi.release()
		# æœ€å„ªè§£å­˜å…¥æ­·å²
		self.history.append(best_response)
		# æœ€å„ªè§£è½‰ç‚ºç¹é«”è¼¸å‡º
		text = ''.join(tokenizer.convert_ids_to_tokens(best_response))
		if text == 'å›¾ç‰‡è¯„è®º':
			text = 'ğŸ˜­ï¸ğŸ˜­ï¸ğŸ˜­ï¸ğŸ˜­ï¸ğŸ˜­ï¸ğŸ˜­ï¸'
		else:
			text = opencc_trad.convert(text)
			text = text.replace('åƒ', 'å–«')
		return text

args = set_interact_args()

# å½“ç”¨æˆ·ä½¿ç”¨GPU,å¹¶ä¸”GPUå¯ç”¨æ—¶
args.cuda = torch.cuda.is_available() and not args.no_cuda
device = 'cuda' if args.cuda else 'cpu'
os.environ['CUDA_VISIBLE_DEVICES'] = args.device

# ç¹ç°¡è½‰æ›å™¨
opencc_trad = opencc2.Converter(from_variant='cn', to_variant='hk', with_phrases=False, fast=True)
opencc_simp = opencc2.Converter(from_variant='hk', to_variant='cn', with_phrases=False, fast=True)

# tokenizer
tokenizer = BertTokenizer(vocab_file=args.voca_path)

# å¯¹è¯model
dialogue_model = GPT2LMHeadModel.from_pretrained(args.dialogue_model_path)
dialogue_model.to(device)
dialogue_model.eval()

# äº’ä¿¡æ¯mmi model
mmi_model = GPT2LMHeadModel.from_pretrained(args.mmi_model_path)
mmi_model.to(device)
mmi_model.eval()

def get_response(history):
	input_ids = [tokenizer.cls_token_id]  # æ¯ä¸ªinputä»¥[CLS]ä¸ºå¼€å¤´
	for history_utr in history[-args.max_history_len:]:
		input_ids.extend(history_utr)
		input_ids.append(tokenizer.sep_token_id)
	# ç”¨äºæ‰¹é‡ç”Ÿæˆresponseï¼Œç»´åº¦ä¸º(batch_size,token_len)
	input_ids = [copy.deepcopy(input_ids) for _ in range(args.batch_size)]

	curr_input_tensors = torch.tensor(input_ids).long().to(device)
	generated = []  # äºŒç»´æ•°ç»„ï¼Œç»´åº¦ä¸º(ç”Ÿæˆçš„responseçš„æœ€å¤§é•¿åº¦ï¼Œbatch_size)ï¼Œgenerated[i,j]è¡¨ç¤ºç¬¬jä¸ªresponseçš„ç¬¬iä¸ªtokençš„id
	finish_set = set()  # æ ‡è®°æ˜¯å¦æ‰€æœ‰responseå‡å·²ç”Ÿæˆç»“æŸï¼Œè‹¥ç¬¬iä¸ªresponseç”Ÿæˆç»“æŸï¼Œå³ç”Ÿæˆäº†sep_token_idï¼Œåˆ™å°†iæ”¾å…¥finish_set
	# æœ€å¤šç”Ÿæˆmax_lenä¸ªtoken
	for _ in range(args.max_len):
		outputs = dialogue_model(input_ids=curr_input_tensors)
		next_token_logits = outputs[0][:, -1, :]
		# å¯¹äºå·²ç”Ÿæˆçš„ç»“æœgeneratedä¸­çš„æ¯ä¸ªtokenæ·»åŠ ä¸€ä¸ªé‡å¤æƒ©ç½šé¡¹ï¼Œé™ä½å…¶ç”Ÿæˆæ¦‚ç‡
		for index in range(args.batch_size):
			for token_id in set([token_ids[index] for token_ids in generated]):
				next_token_logits[index][token_id] /= args.repetition_penalty
		next_token_logits = next_token_logits / args.temperature
		# å¯¹äº[UNK]çš„æ¦‚ç‡è®¾ä¸ºæ— ç©·å°ï¼Œä¹Ÿå°±æ˜¯è¯´æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸å¯èƒ½æ˜¯[UNK]è¿™ä¸ªtoken
		for next_token_logit in next_token_logits:
			next_token_logit[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')
			# åŒç†ï¼Œå±è”½èˆ‡ç”·æ€§ç›¸é—œçš„è©å½™
			for c in 'ç”·å¸¥å…¬å“¥å…„å¼Ÿçˆ¸çˆ¹':
				next_token_logit[tokenizer.convert_tokens_to_ids(c)] = -float('Inf')
			# åŒç†ï¼Œå±è”½è©ˆè©
			for c in 'å¦ˆè‡­è‰è‚å—¨æ­»å±éª‚é€¼æ®‹æå‚»å®³å‘¸æ»š':
				next_token_logit[tokenizer.convert_tokens_to_ids(c)] = -float('Inf')
		filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=args.topk, top_p=args.topp)
		# torch.multinomialè¡¨ç¤ºä»å€™é€‰é›†åˆä¸­æ— æ”¾å›åœ°è¿›è¡ŒæŠ½å–num_samplesä¸ªå…ƒç´ ï¼Œæƒé‡è¶Šé«˜ï¼ŒæŠ½åˆ°çš„å‡ ç‡è¶Šé«˜ï¼Œè¿”å›å…ƒç´ çš„ä¸‹æ ‡
		next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)
		# åˆ¤æ–­æ˜¯å¦æœ‰responseç”Ÿæˆäº†[SEP],å°†å·²ç”Ÿæˆäº†[SEP]çš„responseè¿›è¡Œæ ‡è®°
		for index, token_id in enumerate(next_token[:, 0]):
			if token_id == tokenizer.sep_token_id:
				finish_set.add(index)
		# æ£€éªŒæ˜¯å¦æ‰€æœ‰çš„responseå‡å·²ç”Ÿæˆ[SEP]
		finish_flag = True  # æ˜¯å¦æ‰€æœ‰çš„responseå‡å·²ç”Ÿæˆ[SEP]çš„token
		for index in range(args.batch_size):
			if index not in finish_set:  # responseæ‰¹é‡ç”Ÿæˆæœªå®Œæˆ
				finish_flag = False
				break
		if finish_flag:
			break
		generated.append([token.item() for token in next_token[:, 0]])
		# å°†æ–°ç”Ÿæˆçš„tokenä¸åŸæ¥çš„tokenè¿›è¡Œæ‹¼æ¥
		curr_input_tensors = torch.cat((curr_input_tensors, next_token), dim=-1)
	candidate_responses = []  # ç”Ÿæˆçš„æ‰€æœ‰å€™é€‰response
	for batch_index in range(args.batch_size):
		response = []
		for token_index in range(len(generated)):
			if generated[token_index][batch_index] != tokenizer.sep_token_id:
				response.append(generated[token_index][batch_index])
			else:
				break
		candidate_responses.append(response)

	return candidate_responses

def mmi_choice(history, candidate_responses):
	min_loss = float('Inf')
	best_response = ''
	for response in candidate_responses:
		mmi_input_id = [tokenizer.cls_token_id]  # æ¯ä¸ªinputä»¥[CLS]ä¸ºå¼€å¤´
		mmi_input_id.extend(response)
		mmi_input_id.append(tokenizer.sep_token_id)
		for history_utr in reversed(history[-args.max_history_len:]):
			mmi_input_id.extend(history_utr)
			mmi_input_id.append(tokenizer.sep_token_id)
		mmi_input_tensor = torch.tensor(mmi_input_id).long().to(device)
		out = mmi_model(input_ids=mmi_input_tensor, labels=mmi_input_tensor)
		loss = out[0].item()
		if loss < min_loss:
			best_response = response
			min_loss = loss
	return best_response

class TimedDict:
	def __init__(self):
		self.__table = {}
		self.__d = {}

	def check_key(self, key):
		if key in self.__table and self.__table[key] < time.time():
			self.__table.__delitem__(key)
			self.__d.__delitem__(key)

	def __setitem__(self, key, val, timeout=43200):
		self.__table[key] = time.time() + timeout
		self.__d.__setitem__(key, val)

	def __contains__(self, key):
		self.check_key(key)
		return self.__d.__contains__(key)

	def __getitem__(self, key):
		self.check_key(key)
		return self.__d.__getitem__(key)

	def __delitem__(self, key):
		self.check_key(key)
		return self.__d.__delitem__(key)

	def get(self, key):
		self.check_key(key)
		return self.__d.get(key)

class Talk:
	TALK_LIST = TimedDict()

	def start_talk(self, talk_id, text):
		dialog = self.TALK_LIST.get(talk_id)
		if not dialog:
			dialog = InidvidualDialog()
			self.TALK_LIST[talk_id] = dialog
		return dialog.response(text)

	def remove_talk(self, talk_id):
		if talk_id in self.TALK_LIST:
			del self.TALK_LIST[talk_id]

talk_id = 'ID'
t = Talk()
while True:
	m = input('>>> ')
	if not m:
		break
	print(t.start_talk(talk_id, m))
