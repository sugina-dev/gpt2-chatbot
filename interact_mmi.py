import transformers
import torch
import os
import json
import random
import numpy as np
import argparse
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from tqdm import tqdm
from torch.nn import DataParallel
import logging
from transformers.modeling_gpt2 import GPT2Config, GPT2LMHeadModel
from transformers import BertTokenizer
from os.path import join, exists
from itertools import zip_longest, chain
from dataset import MyDataset
from torch.utils.data import Dataset, DataLoader
from torch.nn import CrossEntropyLoss
from sklearn.model_selection import train_test_split
from train import create_model
import torch.nn.functional as F
import copy
import opencc2

PAD = '[PAD]'
pad_id = 0

opencc_trad = opencc2.Converter(from_variant='cn', to_variant='hk', with_phrases=False, fast=True)
opencc_simp = opencc2.Converter(from_variant='hk', to_variant='cn', with_phrases=False, fast=True)

def set_interact_args():
    """
    Sets up the training arguments.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--device', default='0', type=str, required=False, help='ç”Ÿæˆè®¾å¤‡')
    parser.add_argument('--temperature', default=1, type=float, required=False, help='ç”Ÿæˆçš„temperature')
    parser.add_argument('--topk', default=8, type=int, required=False, help='æœ€é«˜ké€‰1')
    parser.add_argument('--topp', default=0, type=float, required=False, help='æœ€é«˜ç§¯ç´¯æ¦‚ç‡')
    parser.add_argument('--model_config', default='config/model_config_dialogue_small.json', type=str, required=False,
                        help='æ¨¡å‹å‚æ•°')
    parser.add_argument('--log_path', default='data/interacting_mmi.log', type=str, required=False,
                        help='interact_mmiæ—¥å¿—å­˜æ”¾ä½ç½®')
    parser.add_argument('--voca_path', default='vocabulary/vocab_small.txt', type=str, required=False, help='é€‰æ‹©è¯åº“')
    parser.add_argument('--dialogue_model_path', default='dialogue_model/', type=str, required=False,
                        help='dialogue_modelè·¯å¾„')
    parser.add_argument('--mmi_model_path', default='mmi_model/', type=str, required=False,
                        help='äº’ä¿¡æ¯mmi_modelè·¯å¾„')
    parser.add_argument('--repetition_penalty', default=1.5, type=float, required=False,
                        help="é‡å¤æƒ©ç½šå‚æ•°ï¼Œè‹¥ç”Ÿæˆçš„å¯¹è¯é‡å¤æ€§è¾ƒé«˜ï¼Œå¯é€‚å½“æé«˜è¯¥å‚æ•°")
    parser.add_argument('--seed', type=int, default=None, help='è®¾ç½®ç§å­ç”¨äºç”Ÿæˆéšæœºæ•°ï¼Œä»¥ä½¿å¾—è®­ç»ƒçš„ç»“æœæ˜¯ç¡®å®šçš„')
    parser.add_argument('--max_len', type=int, default=25, help='æ¯ä¸ªutteranceçš„æœ€å¤§é•¿åº¦,è¶…è¿‡æŒ‡å®šé•¿åº¦åˆ™è¿›è¡Œæˆªæ–­')
    parser.add_argument('--max_history_len', type=int, default=5, help="dialogue historyçš„æœ€å¤§é•¿åº¦")
    parser.add_argument('--no_cuda', action='store_true', help='ä¸ä½¿ç”¨GPUè¿›è¡Œé¢„æµ‹')
    parser.add_argument('--batch_size', type=int, default=5, help='æ‰¹é‡ç”Ÿæˆresponseï¼Œç„¶åç»è¿‡MMIæ¨¡å‹è¿›è¡Œç­›é€‰')
    parser.add_argument('--debug', action='store_true', help='æŒ‡å®šè¯¥å‚æ•°ï¼Œå¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„æ‰€æœ‰å€™é€‰çš„reponseï¼ŒåŠå…¶loss')
    return parser.parse_args()


def create_logger(args):
    """
    å°†æ—¥å¿—è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶å’Œæ§åˆ¶å°
    """
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s')

    # åˆ›å»ºä¸€ä¸ªhandlerï¼Œç”¨äºå†™å…¥æ—¥å¿—æ–‡ä»¶
    file_handler = logging.FileHandler(
        filename=args.log_path)
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)
    logger.addHandler(file_handler)

    # åˆ›å»ºä¸€ä¸ªhandlerï¼Œç”¨äºå°†æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°
    console = logging.StreamHandler()
    console.setLevel(logging.DEBUG)
    console.setFormatter(formatter)
    logger.addHandler(console)

    return logger


def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
    """ Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
        Args:
            logits: logits distribution shape (vocabulary size)
            top_k > 0: keep only top k tokens with highest probability (top-k filtering).
            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).
                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)
    """
    assert logits.dim() == 2
    top_k = min(top_k, logits[0].size(-1))  # Safety check
    if top_k > 0:
        # Remove all tokens with a probability less than the last token of the top-k
        # torch.topk()è¿”å›æœ€åä¸€ç»´æœ€å¤§çš„top_kä¸ªå…ƒç´ ï¼Œè¿”å›å€¼ä¸ºäºŒç»´(values,indices)
        # ...è¡¨ç¤ºå…¶ä»–ç»´åº¦ç”±è®¡ç®—æœºè‡ªè¡Œæ¨æ–­
        for logit in logits:
            indices_to_remove = logit < torch.topk(logit, top_k)[0][..., -1, None]
            logit[indices_to_remove] = filter_value  # å¯¹äºtopkä¹‹å¤–çš„å…¶ä»–å…ƒç´ çš„logitså€¼è®¾ä¸ºè´Ÿæ— ç©·

    if top_p > 0.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)  # å¯¹logitsè¿›è¡Œé€’å‡æ’åº
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

        # Remove tokens with cumulative probability above the threshold
        sorted_indices_to_remove = cumulative_probs > top_p
        # Shift the indices to the right to keep also the first token above the threshold
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        for index, logit in enumerate(logits):
            indices_to_remove = sorted_indices[index][sorted_indices_to_remove[index]]
            logit[indices_to_remove] = filter_value
    return logits


def main():
    args = set_interact_args()
    logger = create_logger(args)
    # å½“ç”¨æˆ·ä½¿ç”¨GPU,å¹¶ä¸”GPUå¯ç”¨æ—¶
    args.cuda = torch.cuda.is_available() and not args.no_cuda
    device = 'cuda' if args.cuda else 'cpu'
    logger.info('using device:{}'.format(device))
    os.environ["CUDA_VISIBLE_DEVICES"] = args.device
    tokenizer = BertTokenizer(vocab_file=args.voca_path)
    # å¯¹è¯model
    dialogue_model = GPT2LMHeadModel.from_pretrained(args.dialogue_model_path)
    dialogue_model.to(device)
    dialogue_model.eval()
    # äº’ä¿¡æ¯mmi model
    mmi_model = GPT2LMHeadModel.from_pretrained(args.mmi_model_path)
    mmi_model.to(device)
    mmi_model.eval()
    history = []
    print('å¼€å§‹å’ŒchatbotèŠå¤©ï¼Œè¾“å…¥CTRL + Zä»¥é€€å‡º')

    while True:
        try:
            text = input("user:")
            text = opencc_simp.convert(text)
            history.append(tokenizer.encode(text))
            input_ids = [tokenizer.cls_token_id]  # æ¯ä¸ªinputä»¥[CLS]ä¸ºå¼€å¤´
            for history_id, history_utr in enumerate(history[-args.max_history_len:]):
                input_ids.extend(history_utr)
                input_ids.append(tokenizer.sep_token_id)
            # ç”¨äºæ‰¹é‡ç”Ÿæˆresponseï¼Œç»´åº¦ä¸º(batch_size,token_len)
            input_ids = [copy.deepcopy(input_ids) for _ in range(args.batch_size)]

            curr_input_tensors = torch.tensor(input_ids).long().to(device)
            generated = []  # äºŒç»´æ•°ç»„ï¼Œç»´åº¦ä¸º(ç”Ÿæˆçš„responseçš„æœ€å¤§é•¿åº¦ï¼Œbatch_size)ï¼Œgenerated[i,j]è¡¨ç¤ºç¬¬jä¸ªresponseçš„ç¬¬iä¸ªtokençš„id
            finish_set = set()  # æ ‡è®°æ˜¯å¦æ‰€æœ‰responseå‡å·²ç”Ÿæˆç»“æŸï¼Œè‹¥ç¬¬iä¸ªresponseç”Ÿæˆç»“æŸï¼Œå³ç”Ÿæˆäº†sep_token_idï¼Œåˆ™å°†iæ”¾å…¥finish_set
            # æœ€å¤šç”Ÿæˆmax_lenä¸ªtoken
            for _ in range(args.max_len):
                outputs = dialogue_model(input_ids=curr_input_tensors)
                next_token_logits = outputs[0][:, -1, :]
                # å¯¹äºå·²ç”Ÿæˆçš„ç»“æœgeneratedä¸­çš„æ¯ä¸ªtokenæ·»åŠ ä¸€ä¸ªé‡å¤æƒ©ç½šé¡¹ï¼Œé™ä½å…¶ç”Ÿæˆæ¦‚ç‡
                for index in range(args.batch_size):
                    for token_id in set([token_ids[index] for token_ids in generated]):
                        next_token_logits[index][token_id] /= args.repetition_penalty
                next_token_logits = next_token_logits / args.temperature
                # å¯¹äº[UNK]çš„æ¦‚ç‡è®¾ä¸ºæ— ç©·å°ï¼Œä¹Ÿå°±æ˜¯è¯´æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸å¯èƒ½æ˜¯[UNK]è¿™ä¸ªtoken
                for next_token_logit in next_token_logits:
                    next_token_logit[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')
                    # åŒç†ï¼Œå±è”½èˆ‡ç”·æ€§ç›¸é—œçš„è©å½™
                    next_token_logit[tokenizer.convert_tokens_to_ids('ç”·')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å¸¥')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å…¬')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å“¥')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å…„')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å¼Ÿ')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('çˆ¸')] = -float('Inf')
                    # åŒç†ï¼Œå±è”½è©ˆè©
                    next_token_logit[tokenizer.convert_tokens_to_ids('å¦ˆ')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('è‡­')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('è‰')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('è‚')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å—¨')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('æ­»')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å±')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('éª‚')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('é€¼')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('æ®‹')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('æ')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å‚»')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å®³')] = -float('Inf')
                    next_token_logit[tokenizer.convert_tokens_to_ids('å‘¸')] = -float('Inf')
                filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=args.topk, top_p=args.topp)
                # torch.multinomialè¡¨ç¤ºä»å€™é€‰é›†åˆä¸­æ— æ”¾å›åœ°è¿›è¡ŒæŠ½å–num_samplesä¸ªå…ƒç´ ï¼Œæƒé‡è¶Šé«˜ï¼ŒæŠ½åˆ°çš„å‡ ç‡è¶Šé«˜ï¼Œè¿”å›å…ƒç´ çš„ä¸‹æ ‡
                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)
                # åˆ¤æ–­æ˜¯å¦æœ‰responseç”Ÿæˆäº†[SEP],å°†å·²ç”Ÿæˆäº†[SEP]çš„resposneè¿›è¡Œæ ‡è®°
                for index, token_id in enumerate(next_token[:, 0]):
                    if token_id == tokenizer.sep_token_id:
                        finish_set.add(index)
                # æ£€éªŒæ˜¯å¦æ‰€æœ‰çš„responseå‡å·²ç”Ÿæˆ[SEP]
                finish_flag = True  # æ˜¯å¦æ‰€æœ‰çš„responseå‡å·²ç”Ÿæˆ[SEP]çš„token
                for index in range(args.batch_size):
                    if index not in finish_set:  # responseæ‰¹é‡ç”Ÿæˆæœªå®Œæˆ
                        finish_flag = False
                        break
                if finish_flag:
                    break
                generated.append([token.item() for token in next_token[:, 0]])
                # å°†æ–°ç”Ÿæˆçš„tokenä¸åŸæ¥çš„tokenè¿›è¡Œæ‹¼æ¥
                curr_input_tensors = torch.cat((curr_input_tensors, next_token), dim=-1)
            candidate_responses = []  # ç”Ÿæˆçš„æ‰€æœ‰å€™é€‰response
            for batch_index in range(args.batch_size):
                response = []
                for token_index in range(len(generated)):
                    if generated[token_index][batch_index] != tokenizer.sep_token_id:
                        response.append(generated[token_index][batch_index])
                    else:
                        break
                candidate_responses.append(response)

            # mmiæ¨¡å‹çš„è¾“å…¥=
            min_loss = float('Inf')
            best_response = ""
            for response in candidate_responses:
                mmi_input_id = [tokenizer.cls_token_id]  # æ¯ä¸ªinputä»¥[CLS]ä¸ºå¼€å¤´
                mmi_input_id.extend(response)
                mmi_input_id.append(tokenizer.sep_token_id)
                for history_utr in reversed(history[-args.max_history_len:]):
                    mmi_input_id.extend(history_utr)
                    mmi_input_id.append(tokenizer.sep_token_id)
                mmi_input_tensor = torch.tensor(mmi_input_id).long().to(device)
                out = mmi_model(input_ids=mmi_input_tensor, labels=mmi_input_tensor)
                loss = out[0].item()
                if loss < min_loss:
                    best_response = response
                    min_loss = loss
            history.append(best_response)
            text = tokenizer.convert_ids_to_tokens(best_response)
            text = "".join(text)
            if text == 'å›¾ç‰‡è¯„è®º':
                text = 'ğŸ˜­ï¸ğŸ˜­ï¸ğŸ˜­ï¸ğŸ˜­ï¸ğŸ˜­ï¸ğŸ˜­ï¸'
            else:
                text = opencc_trad.convert(text)
            print("chatbot:" + text)
        except KeyboardInterrupt:
            break


if __name__ == '__main__':
    main()
